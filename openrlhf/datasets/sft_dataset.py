from typing import Callable

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
import os

from .utils import zero_pad_sequences
from openrlhf.gameenv.chatutils import make_default_ipt


def preprocess_data(
    data, input_template=None, input_key="input", output_key=None, apply_chat_template=None, multiturn=False
):
    if apply_chat_template:
        if output_key:
            prompt_message = data[input_key]
            response_message = data[output_key]

            if isinstance(prompt_message, str) and isinstance(response_message, str):
                prompt_message = [{"role": "user", "content": prompt_message}]
                response_message = [{"role": "assistant", "content": response_message}]

            prompt = apply_chat_template(prompt_message, tokenize=False, add_generation_prompt=True)
            response = apply_chat_template(prompt_message + response_message, tokenize=False)[len(prompt) :]
        else:
            prompt = apply_chat_template(data[input_key][:-1], tokenize=False, add_generation_prompt=True)
            response = apply_chat_template(data[input_key], tokenize=False)[len(prompt) :]
    else:
        prompt = data[input_key]
        if input_template:
            prompt = input_template.format(prompt)
        # output_key is None for continue pretrain
        response = data[output_key] if output_key else ""
    return prompt, response

class mtSFTDataset(Dataset):
    """
    ä¸“é—¨å¤„ç†å¤šè½®å¯¹è¯æ•°æ®çš„ SFT æ•°æ®é›†

    å‚æ•°:
        dataset: åŸå§‹æ•°æ®é›†ï¼Œæ¯ä¸ªæ ·æœ¬ä¸­ data[input_key] ä¸ºå¯¹è¯è½®æ¬¡åˆ—è¡¨
        tokenizer: ç”¨äºç¼–ç æ–‡æœ¬çš„ tokenizerï¼ŒåŒæ—¶éœ€è¦åŒ…å« apply_chat_template æ–¹æ³•
        max_length: è¾“å…¥æœ€å¤§é•¿åº¦
        strategy: åŒ…å«é…ç½®ä¿¡æ¯ï¼Œå…¶ args ä¸­åº”æä¾› input_key å’Œ tokenizer_chat_template ç­‰å‚æ•°
        input_template: å¯é€‰çš„æ–‡æœ¬æ¨¡æ¿
    """

    def __init__(
        self,
        dataset,
        tokenizer: Callable,
        max_length: int,
        strategy,
        input_template=None,
        pretrain_mode=False,
        num_processors=8,  # Specify the number of processors you want to use
        multiple_of=1,
        multiturn=True,
    ) -> None:
        assert multiturn, "åªèƒ½å¤„ç†å¤šè½®å¯¹è¯æ•°æ®"

        super().__init__()
        self.tokenizer = tokenizer
        self.strategy = strategy
        self.pretrain_mode = pretrain_mode
        self.max_length = max_length
        self.multiple_of = multiple_of
        self.multiturn = multiturn  # å›ºå®šä¸º Trueï¼Œå¤šè½®å¯¹è¯

        # self.input_template = input_template
        self.input_key = getattr(self.strategy.args, "input_key", "input")
        # å¯¹äºå¤šè½®å¯¹è¯ï¼Œä¸å†ä½¿ç”¨ output_key
        self.apply_chat_template = getattr(self.strategy.args, "apply_chat_template", True)
        if self.apply_chat_template:
            self.apply_chat_template = self.tokenizer.apply_chat_template
            tokenizer_chat_template = getattr(self.strategy.args, "tokenizer_chat_template", None)
            if tokenizer_chat_template:
                self.tokenizer.chat_template = tokenizer_chat_template

        # å¹¶è¡Œé¢„å¤„ç†æ•°æ®ï¼šåˆ©ç”¨ map æ–¹æ³•å¤„ç†æ‰€æœ‰æ ·æœ¬
        processed_dataset = dataset.map(
            self.process_data,
            remove_columns=dataset.column_names,
            num_proc=num_processors,
        )
        processed_dataset = processed_dataset.filter(lambda x: x["prompt"] is not None)

        self.prompts = processed_dataset["prompt"]
        self.response_ranges = processed_dataset["response_ranges"]
        self.importants = processed_dataset["important"]
        # if self.multiturn and os.getppid() == os.getpid():
        #     self.print_sample()


    def process_data(self, data):
        # å¯¹äºå¤šè½®æ•°æ®ï¼Œå‡è®¾ data[self.input_key] ä¸ºåŒ…å«å¯¹è¯è½®æ¬¡çš„åˆ—è¡¨
        ipt = data.get("ipt", None)
        prompt, response_ranges, ipts = preprocess_mtdata(
            messages=data[self.input_key],
            msg_ipt=ipt,
            apply_chat_template=self.apply_chat_template,
            max_length=self.max_length,
            tokenizer=self.tokenizer,
        )
        return {"prompt": prompt, "response_ranges":response_ranges, "important": ipts}

    def __len__(self):
        return len(self.prompts)

    def __getitem__(self, idx):
        final_text = self.prompts[idx]
        response_ranges = self.response_ranges[idx]
        ipt = self.importants[idx]
        
        final_text = final_text.rstrip("\n")
        if not final_text.endswith(self.tokenizer.eos_token):
            final_text += self.tokenizer.eos_token

        # å¯¹æœ€ç»ˆæ–‡æœ¬è°ƒç”¨ tokenizerï¼ˆä»…è°ƒç”¨ä¸€æ¬¡ï¼‰å¾—åˆ° input_ids ä¸ attention_mask
        input_token = self.tokenizer(
            final_text,
            max_length=self.max_length,
            padding=False,
            truncation=True,
            return_tensors="pt",
            add_special_tokens=False,
        )
        input_ids = input_token["input_ids"][0]  # tensor, shape ä¸º (seq_len,)
        attention_mask = input_token["attention_mask"][0]
        prompt_ids_len = attention_mask.int().sum().item()
        seq_len = input_ids.shape[0]
        # æ„é€  label åºåˆ—ï¼Œåˆå§‹å…¨éƒ¨ä¸º -100
        labels = [-100] * seq_len
        impts = [0] * seq_len

        for (start, end), imp in zip(response_ranges, ipt):
            for i in range(start, min(end, seq_len)):
                labels[i] = input_ids[i].item()
                impts[i] = imp

        info = {
            "input": final_text,
            "labels": torch.tensor(labels),
            "important": torch.tensor(impts)
        }
        return prompt_ids_len, input_ids, attention_mask, info
    
    def collate_fn(self, item_list):
        prompt_ids_lens = []
        input_ids_list = []
        attention_masks_list = []
        labels_list = []
        ipts_list = []
        # infos = {"input": [], "labels": []}

        for prompt_ids_len, input_ids, attention_mask, info in item_list:
            prompt_ids_lens.append(prompt_ids_len)
            input_ids_list.append(input_ids)
            attention_masks_list.append(attention_mask)
            labels_list.append(info["labels"])
            ipts_list.append(info["important"])
            # infos["input"].append(info["input"])

        # å¯¹ input_ids å’Œ attention_masks ä½¿ç”¨å³ä¾§è¡¥é½
        input_ids_padded = zero_pad_sequences(input_ids_list, "right", self.tokenizer.pad_token_id)
        attention_masks_padded = zero_pad_sequences(attention_masks_list, "right")
        # å¯¹ labels è¿›è¡Œè¡¥é½ï¼Œæ³¨æ„å¡«å……å€¼ä¸º -100
        labels_padded = zero_pad_sequences(labels_list, "right", -100)
        ipts_padded = zero_pad_sequences(ipts_list, "right", 0)
        # infos["labels"] = labels_padded

        return input_ids_padded, attention_masks_padded, labels_padded, ipts_padded
    
    def print_sample(self, index=None):        
        # âœ… åªå…è®¸ rank 0 æ‰“å°ï¼ˆåˆ†å¸ƒå¼ä¸»è¿›ç¨‹ï¼‰
        if not self.strategy.is_rank_0(): 
            return  # éä¸»è¿›ç¨‹ï¼Œç›´æ¥è·³è¿‡
        
        if index is None:
            import random
            index = random.randint(0, len(self)-1)        
        _, input_ids, _, info = self[index]  # è°ƒç”¨ __getitem__

        # è·å–åŸå§‹ input æ–‡æœ¬
        # input_text = info["input"]
        # print("ğŸ”¹ Input:")
        # print(input_text)
        # print()
        
        # ä¿®æ”¹ input_idsï¼šåœ¨æ¯ä¸ª token åæ’å…¥ä¸€ä¸ª 2
        input_ids_modified = []
        for token_id in input_ids.tolist():
            input_ids_modified.extend([token_id, 2])  # æ’å…¥ token å’Œ 2

        # è§£ç ä¿®æ”¹åçš„ input_ids åºåˆ—
        decoded_modified_input = self.tokenizer.decode(input_ids_modified, skip_special_tokens=False)
        print("ğŸ”¹ Input:(with # inserted as cut):")
        print(decoded_modified_input)

        # è§£ç å¯è§†åŒ– label åºåˆ—: è·å– labelsï¼Œå¹¶æ›¿æ¢ -100 ä¸º 2ï¼ˆæˆ–å…¶ä»–å¯è§†åŒ–ç¬¦å·ï¼‰
        labels = info["labels"].tolist()
        visible_labels = [2 if x == -100 else x for x in labels]
        decoded = self.tokenizer.decode(visible_labels, skip_special_tokens=False)
        print("ğŸ”¹ Labels (decoded, -100 â†’ #):")
        print(decoded)
        print()
        # å¯è§†åŒ– important åºåˆ—
        ipts = info["important"].tolist()
        print("ğŸ”¹Important:")
        print(ipts)
        print()

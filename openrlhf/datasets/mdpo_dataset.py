from typing import Callable

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset

from .utils import exist_and_not_none, zero_pad_sequences

class DPOMultiturnDataset(Dataset):
    """
    ä¸“é—¨å¤„ç† DPO æ•°æ®é›†çš„å¤šè½®å¯¹è¯æ•°æ®ï¼Œæ•°æ®ä¸­æ¯æ¡æ ·æœ¬åŒ…å«3ç»„ messages:
      - history
      - chosen
      - rejected

    å¤„ç†æµç¨‹ï¼š
      1. åˆ†åˆ«å°† history ä¸ chosen æ‹¼æ¥ï¼Œå’Œ history ä¸ rejected æ‹¼æ¥ï¼Œç”Ÿæˆä¸¤æ®µæ–‡æœ¬ï¼›
      2. ä»…ä¸º chosen ä¸ rejected éƒ¨åˆ†ä¸­çš„ assistant å›ç­”è®¡ç®— token èŒƒå›´ï¼Œä½œä¸º response_rangesï¼›
      3. process æ¯æ¡æ•°æ®è¿”å›ï¼š
           {
             "chosen": chosen_text, 
             "reject": reject_text, 
             "info": {"chosen_response_ranges": chosen_response_ranges,
                      "reject_response_ranges": reject_response_ranges}
           }
      4. __getitem__ è¿”å›ï¼š
           (
              chosen_token["input_ids"],
              chosen_token["attention_mask"],
              reject_token["input_ids"],
              reject_token["attention_mask"],
              info,  # åŒ…å« chosen_text, reject_text, chosen_response_ranges, reject_response_ranges
           )
    """

    def __init__(
        self,
        dataset,
        tokenizer: Callable,
        max_length: int,
        strategy,
        input_template=None,
        pretrain_mode=False,
        num_processors=8,
        multiple_of=1,
        multiturn=True,
    ) -> None:
        # å›ºå®šå¤šè½®å¯¹è¯
        assert multiturn, "åªèƒ½å¤„ç†å¤šè½®å¯¹è¯æ•°æ®"
        super().__init__()
        self.tokenizer = tokenizer
        self.strategy = strategy
        self.pretrain_mode = pretrain_mode
        self.max_length = max_length
        self.multiple_of = multiple_of
        self.multiturn = multiturn

        self.input_template = input_template
        # æ­¤å¤„å‡å®šæ•°æ®ä¸­çš„å¯¹è¯ä¿¡æ¯åˆ†åˆ«å­˜å‚¨åœ¨ "history", "chosen", "rejected" å­—æ®µä¸­
        # å¯æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´
        self.input_key_history = getattr(self.strategy.args, "input_key_history", "history")
        self.input_key_chosen = getattr(self.strategy.args, "input_key_chosen", "chosen")
        self.input_key_reject = getattr(self.strategy.args, "input_key_reject", "rejected")
        self.apply_chat_template = getattr(self.strategy.args, "apply_chat_template", False)
        if self.apply_chat_template:
            self.apply_chat_template = self.tokenizer.apply_chat_template
            tokenizer_chat_template = getattr(self.strategy.args, "tokenizer_chat_template", None)
            if tokenizer_chat_template:
                self.tokenizer.chat_template = tokenizer_chat_template

        # å¹¶è¡Œé¢„å¤„ç†æ‰€æœ‰æ ·æœ¬
        processed_dataset = dataset.map(
            self.process_data,
            remove_columns=dataset.column_names,
            num_proc=num_processors,
        )
        # è¿‡æ»¤å¤„ç†å¤±è´¥çš„æ ·æœ¬
        processed_dataset = processed_dataset.filter(lambda x: x["chosen"] is not None and x["reject"] is not None)
        self.chosen_texts = processed_dataset["chosen"]
        self.reject_texts = processed_dataset["reject"]
        self.infos = processed_dataset["info"]
        # å¯é€‰ï¼šå±•ç¤ºä¸€ä¸ªæ ·æœ¬
        self.print_sample()

    def preprocess_dpo_messages(self, history, responses, max_length, tokenize_fn):
        """
        æ‹¼æ¥ history ä¸ responsesï¼Œä¸”ä»…ä¸º responses éƒ¨åˆ†ä¸­çš„ assistant å›ç­”è®¡ç®— token èŒƒå›´ã€‚
        tokenize_fn: ç”¨äº tokenizer ç¼–ç çš„æ–¹æ³•ï¼ˆapply_chat_templateï¼‰
        è¿”å›ï¼š (full_text, response_ranges)
        """
        messages = history + responses
        response_ranges = []
        # ä»…å¤„ç† responses éƒ¨åˆ†ï¼Œä» index = len(history) å¼€å§‹
        for idx in range(len(history), len(messages)):
            message = messages[idx]
            if message["role"] == "assistant":
                # prompt_part ä¸º assistant å›ç­”ä¹‹å‰çš„å†…å®¹ï¼ˆåŒ…å« history ä¸ responses ä¸­ assistant å›ç­”ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
                prompt_part = tokenize_fn(messages[:idx], tokenize=False, add_generation_prompt=True)
                full_part = tokenize_fn(messages[: idx + 1], tokenize=False)
                # assistant éƒ¨åˆ†ä¸ºä¸¤è€…å·®å€¼
                assistant_part = full_part[len(prompt_part):]
                start_idx = self.tokenizer(
                    prompt_part,
                    max_length=max_length,
                    padding=False,
                    truncation=True,
                    return_tensors="pt",
                    add_special_tokens=False,
                )["attention_mask"].int().sum().item()
                assistant_token_count = self.tokenizer(
                    assistant_part,
                    max_length=max_length,
                    padding=False,
                    truncation=True,
                    return_tensors="pt",
                    add_special_tokens=False,
                )["attention_mask"].int().sum().item()
                end_idx = start_idx + assistant_token_count
                response_ranges.append((start_idx, end_idx))
        final_text = tokenize_fn(messages, tokenize=False)
        return final_text, response_ranges

    def process_data(self, data):
        """
        data ä¸­åŒ…å«:
           data[self.input_key_history]: history åˆ—è¡¨
           data[self.input_key_chosen]: chosen åˆ—è¡¨
           data[self.input_key_reject]: rejected åˆ—è¡¨
        è¿”å›:
           {
             "chosen": chosen_text,
             "reject": reject_text,
             "info": {"chosen_response_ranges": chosen_response_ranges,
                      "reject_response_ranges": reject_response_ranges}
           }
        """
        history = data[self.input_key_history]
        chosen_msgs = data[self.input_key_chosen]
        reject_msgs = data[self.input_key_reject]

        # æ‹¼æ¥ history+chosenï¼Œå¹¶è®¡ç®— chosen éƒ¨åˆ† assistant å›ç­”çš„ token èŒƒå›´
        chosen_text, chosen_response_ranges = self.preprocess_dpo_messages(
            history, chosen_msgs, self.max_length, self.apply_chat_template
        )
        # æ‹¼æ¥ history+rejectedï¼Œå¹¶è®¡ç®— rejected éƒ¨åˆ† assistant å›ç­”çš„ token èŒƒå›´
        reject_text, reject_response_ranges = self.preprocess_dpo_messages(
            history, reject_msgs, self.max_length, self.apply_chat_template
        )

        # å¯¹æ–‡æœ¬è¿›è¡Œ token ç¼–ç ï¼Œåˆ¤æ–­é•¿åº¦æ˜¯å¦è¶…é™
        chosen_token = self.tokenizer(
            chosen_text,
            max_length=self.max_length,
            padding=False,
            truncation=True,
            return_tensors="pt",
            add_special_tokens=False,
        )
        chosen_len = chosen_token["attention_mask"].int().sum().item()
        reject_token = self.tokenizer(
            reject_text,
            max_length=self.max_length,
            padding=False,
            truncation=True,
            return_tensors="pt",
            add_special_tokens=False,
        )
        reject_len = reject_token["attention_mask"].int().sum().item()

        if (not chosen_text) or (chosen_len >= self.max_length - 2) or (not reject_text) or (reject_len >= self.max_length - 2):
            print(f"to long{'*'*100}\n{chosen_text}\n\n{reject_text}")
            chosen_text, reject_text = None, None
            chosen_response_ranges, reject_response_ranges = None, None

        return {
            "chosen": chosen_text, 
            "reject": reject_text, 
            "info": {
                "chosen_response_ranges": chosen_response_ranges,
                "reject_response_ranges": reject_response_ranges
            }
        }

    def __len__(self):
        return len(self.chosen_texts)

    def __getitem__(self, idx):
        chosen_text = self.chosen_texts[idx]
        reject_text = self.reject_texts[idx]
        infos = self.infos[idx]
        chosen_ranges = infos["chosen_response_ranges"]
        reject_ranges = infos["reject_response_ranges"]

        # ç¡®ä¿æ–‡æœ¬ä»¥ eos_token ç»“å°¾
        if not chosen_text.rstrip("\n").endswith(self.tokenizer.eos_token):
            chosen_text = chosen_text.rstrip("\n") + self.tokenizer.eos_token
        if not reject_text.rstrip("\n").endswith(self.tokenizer.eos_token):
            reject_text = reject_text.rstrip("\n") + self.tokenizer.eos_token

        # åˆ†åˆ«ç¼–ç  chosen ä¸ reject æ–‡æœ¬ï¼ˆä»…è°ƒç”¨ä¸€æ¬¡ tokenizerï¼‰
        chosen_token = self.tokenizer(
            chosen_text,
            max_length=self.max_length,
            padding=False,
            truncation=True,
            return_tensors="pt",
            add_special_tokens=False,
        )
        reject_token = self.tokenizer(
            reject_text,
            max_length=self.max_length,
            padding=False,
            truncation=True,
            return_tensors="pt",
            add_special_tokens=False,
        )
        chosen_input_ids = chosen_token["input_ids"][0]
        chosen_attention_mask = chosen_token["attention_mask"][0]
        reject_input_ids = reject_token["input_ids"][0]
        reject_attention_mask = reject_token["attention_mask"][0]

        # info ä¸­ä¿ç•™æ–‡æœ¬å’Œ response_rangesï¼Œlabels å¯åœ¨åç»­é˜¶æ®µæ ¹æ® response_ranges æ„å»º
        info = {
            "chosen_text": chosen_text,
            "reject_text": reject_text,
            "chosen_response_ranges": chosen_ranges,
            "reject_response_ranges": reject_ranges,
        }
        return (
            chosen_input_ids,
            chosen_attention_mask,
            reject_input_ids,
            reject_attention_mask,
            info,
        )

    def collate_fn(self, batch):
        # batch ä¸º listï¼Œæ¯é¡¹ä¸º __getitem__ è¿”å›çš„å…ƒç»„
        chosen_ids_list = []
        chosen_masks_list = []
        reject_ids_list = []
        reject_masks_list = []
        infos = {"chosen_text": [], "reject_text": [], "chosen_response_ranges": [], "reject_response_ranges": []}

        for chosen_ids, chosen_mask, reject_ids, reject_mask, info in batch:
            chosen_ids_list.append(chosen_ids)
            chosen_masks_list.append(chosen_mask)
            reject_ids_list.append(reject_ids)
            reject_masks_list.append(reject_mask)
            infos["chosen_text"].append(info["chosen_text"])
            infos["reject_text"].append(info["reject_text"])
            infos["chosen_response_ranges"].append(info["chosen_response_ranges"])
            infos["reject_response_ranges"].append(info["reject_response_ranges"])

        # åˆ©ç”¨å³ä¾§è¡¥é½
        chosen_ids_padded = zero_pad_sequences(chosen_ids_list, "right", self.tokenizer.pad_token_id)
        chosen_masks_padded = zero_pad_sequences(chosen_masks_list, "right")
        reject_ids_padded = zero_pad_sequences(reject_ids_list, "right", self.tokenizer.pad_token_id)
        reject_masks_padded = zero_pad_sequences(reject_masks_list, "right")
        # response_ranges ä¿æŒåŸæ ·ï¼Œä¸è¿›è¡Œ padding
        return chosen_ids_padded, chosen_masks_padded, reject_ids_padded, reject_masks_padded, infos

    def print_sample(self, index=0):
        # å±•ç¤ºä¸€ä¸ªæ ·æœ¬çš„éƒ¨åˆ†ä¿¡æ¯ï¼Œæ–¹ä¾¿è°ƒè¯•
        chosen_ids, chosen_mask, reject_ids, reject_mask, info = self[index]
        print("ğŸ”¹ Chosen Input:")
        print(info["chosen_text"])
        # æ ¹æ® chosen_input_ids å’Œ response_ranges æ„é€  labels æ ·ä¾‹
        chosen_labels = [-100] * len(chosen_ids)
        for start, end in info["chosen_response_ranges"]:
            for i in range(start, min(end, len(chosen_ids))):
                chosen_labels[i] = chosen_ids[i].item()
        visible_chosen = [2 if x == -100 else x for x in chosen_labels]
        print("\nğŸ”¹ Chosen Labels (assistantéƒ¨åˆ†å·²æ ‡è®°):")
        print(self.tokenizer.decode(visible_chosen, skip_special_tokens=False))
        print("\nğŸ”¹ Reject Input:")
        print(info["reject_text"])
        # æ ¹æ® reject_input_ids å’Œ response_ranges æ„é€  labels æ ·ä¾‹
        reject_labels = [-100] * len(reject_ids)
        for start, end in info["reject_response_ranges"]:
            for i in range(start, min(end, len(reject_ids))):
                reject_labels[i] = reject_ids[i].item()
        visible_reject = [2 if x == -100 else x for x in reject_labels]
        print("\nğŸ”¹ Reject Labels (assistantéƒ¨åˆ†å·²æ ‡è®°):")
        print(self.tokenizer.decode(visible_reject, skip_special_tokens=False))
